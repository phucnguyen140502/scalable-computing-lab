{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import row_number, lit\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()"
      ],
      "metadata": {
        "id": "T0X-zhETTM0a"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) How to convert the index of a PySpark DataFrame into a column?\n",
        "\n",
        "```python\n",
        "# Input: Assuming df is your DataFrame\n",
        "df = spark.createDataFrame([\n",
        "(\"Alice\", 1),\n",
        "(\"Bob\", 2),\n",
        "(\"Charlie\", 3),\n",
        "], [\"Name\", \"Value\"])\n",
        "\n",
        "df.show()\n",
        "\n",
        "+-------+-----+\n",
        "| Name|Value|\n",
        "+-------+-----+\n",
        "| Alice| 1|\n",
        "| Bob| 2|\n",
        "|Charlie| 3|\n",
        "+-------+-----+\n",
        "\n",
        "# Output:\n",
        "+-------+-----+-----+\n",
        "| Name|Value|index|\n",
        "+-------+-----+-----+\n",
        "| Alice| 1| 0|\n",
        "| Bob| 2| 1|\n",
        "|Charlie| 3| 2|\n",
        "+-------+-----+-----+\n",
        "```"
      ],
      "metadata": {
        "id": "PqLMtw2_M1sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([\n",
        "(\"Alice\", 1),\n",
        "(\"Bob\", 2),\n",
        "(\"Charlie\", 3),\n",
        "], [\"Name\", \"Value\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5cfWguJS4Qa",
        "outputId": "b86d78b6-c9ba-4558-83c2-965d3a17fcfd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   Name|Value|\n",
            "+-------+-----+\n",
            "|  Alice|    1|\n",
            "|    Bob|    2|\n",
            "|Charlie|    3|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = Window.orderBy(lit(1))\n",
        "df = df.withColumn(\"index\", row_number().over(w) - 1)"
      ],
      "metadata": {
        "id": "uf6BYXHETAGH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_rzERk_TuhX",
        "outputId": "f10116a3-9ff5-4a51-d11b-0b091df4aeac"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+\n",
            "|   Name|Value|index|\n",
            "+-------+-----+-----+\n",
            "|  Alice|    1|    0|\n",
            "|    Bob|    2|    1|\n",
            "|Charlie|    3|    2|\n",
            "+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) How to get the minimum, 25th percentile, median, 75th, and max of a numeric column?\n",
        "\n",
        "Compute the minimum, 25th percentile, median, 75th, and maximum of column `Age`\n",
        "\n",
        "```python\n",
        "# Create a sample DataFrame\n",
        "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "```"
      ],
      "metadata": {
        "id": "EnxZlhnhNRzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])"
      ],
      "metadata": {
        "id": "H1QGR_ADVDtO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9lYYJowVHig",
        "outputId": "d9e6282d-7b88-466c-dff7-3dbd70eecbf1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+\n",
            "|Name|Age|\n",
            "+----+---+\n",
            "|   A| 10|\n",
            "|   B| 20|\n",
            "|   C| 30|\n",
            "|   D| 40|\n",
            "|   E| 50|\n",
            "|   F| 15|\n",
            "|   G| 28|\n",
            "|   H| 54|\n",
            "|   I| 41|\n",
            "|   J| 86|\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.summary(\"min\", \"25%\", \"50%\", \"75%\", \"max\").select(\"summary\",\"Age\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYhou4ktVKlk",
        "outputId": "1ca1e9ee-06f6-4ed7-8a05-df726142a65c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|summary|Age|\n",
            "+-------+---+\n",
            "|    min| 10|\n",
            "|    25%| 20|\n",
            "|    50%| 30|\n",
            "|    75%| 50|\n",
            "|    max| 86|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Calculte the frequency counts of each unique value\n",
        "\n",
        "```python\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='Mary', job='Scientist'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Scientist'),\n",
        "Row(name='Sam', job='Doctor'),\n",
        "]\n",
        "\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "```"
      ],
      "metadata": {
        "id": "MWhBlLpENtVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='Mary', job='Scientist'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Scientist'),\n",
        "Row(name='Sam', job='Doctor'),\n",
        "]\n",
        "\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(data)"
      ],
      "metadata": {
        "id": "ZzhDUapZWe8M"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDQP4szCWkLV",
        "outputId": "9be721fc-3d27-4868-b7e3-9c98231c9941"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|name|      job|\n",
            "+----+---------+\n",
            "|John| Engineer|\n",
            "|John| Engineer|\n",
            "|Mary|Scientist|\n",
            "| Bob| Engineer|\n",
            "| Bob| Engineer|\n",
            "| Bob|Scientist|\n",
            "| Sam|   Doctor|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"name\",\"job\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iohY-MdzWl3q",
        "outputId": "a48d2aed-925e-4264-c0ad-540e92916d98"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+-----+\n",
            "|name|      job|count|\n",
            "+----+---------+-----+\n",
            "|Mary|Scientist|    1|\n",
            "|John| Engineer|    2|\n",
            "| Sam|   Doctor|    1|\n",
            "| Bob| Engineer|    2|\n",
            "| Bob|Scientist|    1|\n",
            "+----+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) How to keep only top 2 most frequent values as it is and replace everything else as `Other`?\n",
        "\n",
        "```python\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='Mary', job='Scientist'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Scientist'),\n",
        "Row(name='Sam', job='Doctor'),\n",
        "]\n",
        "\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "```"
      ],
      "metadata": {
        "id": "0qoM05JdN-Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='Mary', job='Scientist'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Scientist'),\n",
        "Row(name='Sam', job='Doctor'),\n",
        "]\n",
        "\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(data)"
      ],
      "metadata": {
        "id": "UvRLqe1rXun7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD6VmSwfXxKs",
        "outputId": "c6e9aa93-86f8-428d-fa10-d147e063183e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|name|      job|\n",
            "+----+---------+\n",
            "|John| Engineer|\n",
            "|John| Engineer|\n",
            "|Mary|Scientist|\n",
            "| Bob| Engineer|\n",
            "| Bob| Engineer|\n",
            "| Bob|Scientist|\n",
            "| Sam|   Doctor|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"name\",\"job\").count().orderBy(\"count\", ascending=False).limit(2).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPTstAy4XyVa",
        "outputId": "7d6e4580-29ee-4d8e-e113-d01ee1b2bea0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+-----+\n",
            "|name|     job|count|\n",
            "+----+--------+-----+\n",
            "|John|Engineer|    2|\n",
            "| Bob|Engineer|    2|\n",
            "+----+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) How to rename columns of a PySpark DataFrame using two lists – one containing the old column names and the other containing the new column names?\n",
        "\n",
        "```python\n",
        "# suppose you have the following DataFrame\n",
        "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
        "\n",
        "# old column names\n",
        "old_names = [\"col1\", \"col2\", \"col3\"]\n",
        "\n",
        "# new column names\n",
        "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
        "```"
      ],
      "metadata": {
        "id": "mtVhEU1qOLt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# suppose you have the following DataFrame\n",
        "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
        "\n",
        "# old column names\n",
        "old_names = [\"col1\", \"col2\", \"col3\"]\n",
        "\n",
        "# new column names\n",
        "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]"
      ],
      "metadata": {
        "id": "3IFYh8yxbL3G"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVCDWlsYbQ0o",
        "outputId": "ce955b9c-a11b-4742-aa42-e9a6b6dc29c9"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|   1|   2|   3|\n",
            "|   4|   5|   6|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for old_names, new_names in zip(old_names, new_names):\n",
        "    df = df.withColumnRenamed(old_names, new_names)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_9JZHwNbT8-",
        "outputId": "47d66f15-19f2-4007-c2eb-88fddaf834ec"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+--------+\n",
            "|new_col1|new_col2|new_col3|\n",
            "+--------+--------+--------+\n",
            "|       1|       2|       3|\n",
            "|       4|       5|       6|\n",
            "+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) How to bin a numeric list to 10 groups of equal size?\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import rand\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "# Create a DataFrame with a single column \"values\" filled with random numbers\n",
        "num_items = 100\n",
        "df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))\n",
        "```"
      ],
      "metadata": {
        "id": "E4OfiEE9OiA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "UTmJ27kzMuyC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import rand\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "# Create a DataFrame with a single column \"values\" filled with random numbers\n",
        "num_items = 100\n",
        "df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TtYXMBfbrAH",
        "outputId": "b1f8006b-9ec8-49ba-8696-497f0d3a15a4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|              values|\n",
            "+--------------------+\n",
            "|   0.619189370225301|\n",
            "|  0.5096018842446481|\n",
            "|  0.8325259388871524|\n",
            "| 0.26322809041172357|\n",
            "|  0.6702867696264135|\n",
            "|  0.5173283545794627|\n",
            "|  0.9991441647585968|\n",
            "| 0.06993233728279169|\n",
            "|  0.9696695610826327|\n",
            "|  0.7959575617927873|\n",
            "|  0.4484250584033179|\n",
            "|  0.6793959570375868|\n",
            "|  0.3724113862805264|\n",
            "|   0.832609472539921|\n",
            "|  0.7479557402720448|\n",
            "|  0.7216183163402288|\n",
            "|0.016051221049720343|\n",
            "|  0.6307120027798567|\n",
            "|    0.07537082371587|\n",
            "|   0.838930558220017|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand, ntile"
      ],
      "metadata": {
        "id": "PDLsb6EPdSEX"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WindowSpec = Window.orderBy(\"values\")\n",
        "df.withColumn(\"bin\", ntile(10).over(WindowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPfzqhR5bsDm",
        "outputId": "698e6b84-60dc-4d97-897b-033160f3623a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---+\n",
            "|              values|bin|\n",
            "+--------------------+---+\n",
            "|0.005039492476539786|  1|\n",
            "|0.010815371607771573|  1|\n",
            "|0.016051221049720343|  1|\n",
            "|0.054599328832547256|  1|\n",
            "| 0.06476522637900317|  1|\n",
            "| 0.06993233728279169|  1|\n",
            "|  0.0733222570891463|  1|\n",
            "| 0.07531261247891552|  1|\n",
            "|    0.07537082371587|  1|\n",
            "| 0.07705198481765851|  1|\n",
            "| 0.09793090948613814|  2|\n",
            "| 0.11388471205143302|  2|\n",
            "| 0.12847898035637806|  2|\n",
            "| 0.13800057515473785|  2|\n",
            "| 0.16191945645714856|  2|\n",
            "|  0.1663672731939081|  2|\n",
            "|  0.1732080573424314|  2|\n",
            "|  0.1882393785257075|  2|\n",
            "| 0.20593872923627632|  2|\n",
            "|  0.2073428376111074|  2|\n",
            "+--------------------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}