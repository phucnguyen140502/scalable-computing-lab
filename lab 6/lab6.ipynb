{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZTQAqfotZoc",
        "outputId": "453ea58f-f0ad-4c66-92a9-f89872cea64e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise I\n",
        "\n",
        "The input is a textual csv file containing the daily value of PM10 for a set of sensors, and in each line of the files has the following format:\n",
        "```sensorId,date,PM10 value (Î¼g/m3)\\n```\n",
        "\n",
        "Here is the example of data:\n",
        "```\n",
        "s1,2016-01-01,20.5\n",
        "s2,2016-01-01,30.1\n",
        "s1,2016-01-02,60.2\n",
        "s2,2016-01-02,20.4\n",
        "s1,2016-01-03,55.5\n",
        "s2,2016-01-03,52.5\n",
        "```\n",
        "\n",
        "You're required to use pyspark to load the file, filter the values and use map/reduce code idea to give the output. The output is a line for each sensor on the standard output.\n",
        "Each line contains a `sensorId` and the list of `dates` with a PM10 values greater than 50 for that sensor. The example output:\n",
        "```\n",
        "(s1, [2016-01-02, 2016-01-03])\n",
        "(s2, [2016-01-03])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "8H58RnChZq0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjD1fivfsyZV",
        "outputId": "23d4d20f-1547-479d-d87d-be7d4b0613cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"PM10Analysis\").getOrCreate()"
      ],
      "metadata": {
        "id": "FZprx3jMtK1W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your file is in 'My Drive' in Google Drive:\n",
        "file_path = \"/content/drive/My Drive/sensor.csv\"  # Update with the correct path\n",
        "\n",
        "sensor_rdd = spark.sparkContext.textFile(file_path)\n",
        "#sensor_rdd.collect()"
      ],
      "metadata": {
        "id": "sekLP_7UtPXx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sensor_rdd_larger_than_50 = sensor_rdd.map(lambda line: line.split(\",\")) \\\n",
        "                       .filter(lambda x: float(x[2]) > 50) \\\n",
        "                       .map(lambda x: (x[0], [x[1]]))\n",
        "for item in sensor_rdd_larger_than_50.collect():\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itiNvh6hr7IQ",
        "outputId": "2e8ff319-b95d-45de-e121-98303e2c2546"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('s1', ['2016-01-02'])\n",
            "('s1', ['2016-01-03'])\n",
            "('s2', ['2016-01-03'])\n",
            "('s1', ['2016-01-04'])\n",
            "('s3', ['2016-01-05'])\n",
            "('s4', ['2016-01-05'])\n",
            "('s3', ['2016-01-06'])\n",
            "('s4', ['2016-01-06'])\n",
            "('s1', ['2016-01-07'])\n",
            "('s2', ['2016-01-07'])\n",
            "('s3', ['2016-01-08'])\n",
            "('s2', ['2016-01-09'])\n",
            "('s3', ['2016-01-09'])\n",
            "('s4', ['2016-01-10'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise II\n",
        "\n",
        "Using the same data of the Exercise I, you're required to get the output: sensors ordered by the number of critical days. Each line of the output contains the number of days with a PM10 values greater than 50 for a sensor `s` and the `sensorId` of sensor `s`.\n",
        "\n",
        "The example of the output:\n",
        "```\n",
        "2, s1\n",
        "1, s2\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Jgu0vQKVbqDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sensor_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qshsredaxiHr",
        "outputId": "9ce3aab2-cf32-42dd-e343-5d51bd729126"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['s1,2016-01-01,20.5',\n",
              " 's2,2016-01-01,30.1',\n",
              " 's1,2016-01-02,60.2',\n",
              " 's2,2016-01-02,20.4',\n",
              " 's1,2016-01-03,55.5',\n",
              " 's2,2016-01-03,52.5',\n",
              " 's1,2016-01-04,55.6',\n",
              " 's2,2016-01-04,49.7',\n",
              " 's3,2016-01-05,50.8',\n",
              " 's4,2016-01-05,53.9',\n",
              " 's3,2016-01-06,57.10',\n",
              " 's4,2016-01-06,54.11',\n",
              " 's1,2016-01-07,51.12',\n",
              " 's2,2016-01-07,53.13',\n",
              " 's3,2016-01-08,62.14',\n",
              " 's4,2016-01-08,42.15',\n",
              " 's2,2016-01-09,62.16',\n",
              " 's3,2016-01-09,55.17',\n",
              " 's4,2016-01-10,56.18']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in sensor_rdd.map(lambda line: line.split(\",\")) \\\n",
        "                       .filter(lambda x: float(x[2]) > 50) \\\n",
        "                       .map(lambda x: (x[0], 1)) \\\n",
        "                       .reduceByKey(lambda x, y: x + y) \\\n",
        "                       .sortBy(lambda x: x[1], ascending=False) \\\n",
        "                       .collect():\n",
        "      print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVYbCEFjxFO5",
        "outputId": "54d51c99-94f1-4496-d056-836515b9c783"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('s1', 4)\n",
            "('s3', 4)\n",
            "('s2', 3)\n",
            "('s4', 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise III\n",
        "\n",
        "In this exercise, you're given an input: A CSV file containing a list of profiles\n",
        "\n",
        "- Header: `name,age,gender`\n",
        "- Each line of the file contains the information about one user\n",
        "\n",
        "The example of input data\n",
        "```\n",
        "name,surname,age\n",
        "Paolo,Garza,42\n",
        "Luca,Boccia,41\n",
        "Maura,Bianchi,16\n",
        "```\n",
        "\n",
        "You're required to use pyspark to load and analyze the data to achieve the output: A CSV file containing one line for each profile. The original age attribute is substituted with a new attributed called rangeage of type String.\n",
        "```\n",
        "rangeage = \"[\" + (age/10)*10 + \"-\" + (age/10)*10+9 + \"]\"\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ADGjGNWKePfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark_df = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()"
      ],
      "metadata": {
        "id": "igKez2B6aPe0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person_df = spark_df.read.csv(\"/content/drive/My Drive/person.csv\", header=True, inferSchema=True)\n",
        "person_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i7PmfnXyuTI",
        "outputId": "def49ada-3587-49c8-d8e3-d3e36cfc1f86"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+---+\n",
            "| name| surname|age|\n",
            "+-----+--------+---+\n",
            "|Paolo|   Garza| 42|\n",
            "| Luca|  Boccia| 41|\n",
            "|Maura| Bianchi| 16|\n",
            "|Alice|   Cochi| 17|\n",
            "|Laura|  Latini| 28|\n",
            "|Paula| Zachini| 19|\n",
            "|Carta|  Cianci| 29|\n",
            "| Rita|Lisatini| 31|\n",
            "+-----+--------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat, lit, floor, col\n",
        "\n",
        "person_df.withColumn(\n",
        "    \"rangeage\",\n",
        "    concat(\n",
        "        lit(\"[\"),\n",
        "        floor(col(\"age\") / 10) * 10,\n",
        "        lit(\"-\"),\n",
        "        floor(col(\"age\") / 10) * 10 + 9,\n",
        "        lit(\"]\")\n",
        "    )\n",
        ").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP5PK1k3zU-_",
        "outputId": "53827859-0d94-446b-d139-18c570d0533b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+---+--------+\n",
            "| name| surname|age|rangeage|\n",
            "+-----+--------+---+--------+\n",
            "|Paolo|   Garza| 42| [40-49]|\n",
            "| Luca|  Boccia| 41| [40-49]|\n",
            "|Maura| Bianchi| 16| [10-19]|\n",
            "|Alice|   Cochi| 17| [10-19]|\n",
            "|Laura|  Latini| 28| [20-29]|\n",
            "|Paula| Zachini| 19| [10-19]|\n",
            "|Carta|  Cianci| 29| [20-29]|\n",
            "| Rita|Lisatini| 31| [30-39]|\n",
            "+-----+--------+---+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}